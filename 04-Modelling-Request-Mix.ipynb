{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import datetime\n",
    "\n",
    "from random import randint\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from random import randint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "\n",
    "\n",
    "from math import sin\n",
    "from math import pi\n",
    "from math import exp\n",
    "from random import randint\n",
    "from random import uniform\n",
    "\n",
    "from numpy import array\n",
    "from math import ceil\n",
    "from math import log10\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import RepeatVector\n",
    "\n",
    "from random import random\n",
    "from numpy import cumsum\n",
    "from numpy import array_equal\n",
    "from keras.layers import Bidirectional\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import re\n",
    "\n",
    "pd.set_option('display.max_columns', 999)\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval transformation functions\n",
    "\n",
    "def most_common(lst): \n",
    "    return max(set(lst), key = lst.count)\n",
    "\n",
    "def reveal_bias(url):\n",
    "    path = url.split('/', 1)[1]\n",
    "    apps = re.sub('[/-]', ' ',path).split()[:-1]\n",
    "    return most_common(apps)\n",
    "\n",
    "def add_bias_labels(col_lst):\n",
    "    for i in range(len(col_lst)):\n",
    "        col_lst[i] = col_lst[i] + \"-bias-\" + reveal_bias(col_lst[i])\n",
    "    return col_lst\n",
    "\n",
    "def create_reqset_dict(col_list, req_list):\n",
    "    reqset_dict = dict(zip(add_bias_labels(col_list),req_list))\n",
    "    return reqset_dict\n",
    "\n",
    "def get_reqset_bias(dictionary):\n",
    "    bias_2 = sum([v for k,v in dictionary.items() if \"-bias-2\" in k])\n",
    "    bias_3 = sum([v for k,v in dictionary.items() if \"-bias-3\" in k])\n",
    "    bias_4 = sum([v for k,v in dictionary.items() if \"-bias-4\" in k])\n",
    "    \n",
    "    return (bias_2, bias_3, bias_4)\n",
    "\n",
    "def get_app_bias_error(X_test, y_test, model):\n",
    "    error_vectors_app_bias = list()\n",
    "    for i in range(len(X_test)):\n",
    "        predict_dict = create_reqset_dict([col for col in test_df.columns if \"CS_\" in col], model.predict(X_test[i].reshape(1,1000,129))[0])\n",
    "        predict_bias = get_reqset_bias(predict_dict)\n",
    "\n",
    "        actual_dict = create_reqset_dict([col for col in test_df.columns if \"CS_\" in col], y[i])\n",
    "        actual_bias = get_reqset_bias(actual_dict)\n",
    "\n",
    "        error_vectors_app_bias.append(np.absolute(np.array(actual_bias) - np.array(predict_bias)))\n",
    "\n",
    "    return error_vectors_app_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_parquet('train.parquet')\n",
    "test_df = pd.read_parquet('test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First half of Test as validation - second half: holdout\n",
    "\n",
    "holdout_df = test_df[:1120000]\n",
    "test_df = test_df[1120000:2240000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test data as validation\n",
    "\n",
    "y_val = array(test_df[[col for col in test_df.columns if \"CS_\" in col]]).reshape(1120000,129)\n",
    "\n",
    "X_val = test_df[[col for col in test_df.columns if \"CS_\" not in col]].to_numpy()\n",
    "X_val = X_val.reshape(1120,1000,129)\n",
    "\n",
    "# Get the thousandth request vector for y\n",
    "\n",
    "sub_y_val = y_val[::1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data\n",
    "\n",
    "y = array(train_df[[col for col in train_df.columns if \"CS_\" in col]]).reshape(2241000,129)\n",
    "\n",
    "X = train_df[[col for col in train_df.columns if \"CS_\" not in col]].to_numpy()\n",
    "X = X.reshape(2241,1000,129)\n",
    "\n",
    "# Get the thousandth request vector for y\n",
    "\n",
    "sub_y = y[::1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test data\n",
    "\n",
    "y_test = array(holdout_df[[col for col in holdout_df.columns if \"CS_\" in col]]).reshape(1120000,129)\n",
    "\n",
    "X_test = holdout_df[[col for col in holdout_df.columns if \"CS_\" not in col]].to_numpy()\n",
    "X_test = X_test.reshape(1120,1000,129)\n",
    "\n",
    "# Get the thousandth request vector for y\n",
    "\n",
    "sub_y_test = y_test[::1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = list()\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "# grid search epochs, batch size and optimizer\n",
    "optimizers = [keras.optimizers.Adam(lr=0.001), keras.optimizers.Adam(lr=0.01), keras.optimizers.Adam(lr=0.1)]\n",
    "epochs = [1,3,5,10,15]\n",
    "batches = [8,16,32,64,128]\n",
    "num_units = [10,25,50,75,100,200]\n",
    "num_layers = [2,3,5,8]\n",
    "\n",
    "\n",
    "for optimizer in optimizers:\n",
    "    for epoch in epochs:\n",
    "        for batch in batches:\n",
    "            for num in num_units:\n",
    "                for layers in num_layers:\n",
    "                    \n",
    "                    model_dict = dict()\n",
    "                    \n",
    "                    model = Sequential()\n",
    "                    model.add(LSTM(num, return_sequences=True, input_shape=(1000, 129)))\n",
    "                    if layers == 3:\n",
    "                        model.add(LSTM(num, return_sequences=True))\n",
    "                    elif layers == 5:\n",
    "                        model.add(LSTM(num, return_sequences=True))\n",
    "                        model.add(LSTM(num, return_sequences=True))\n",
    "                        model.add(LSTM(num, return_sequences=True))\n",
    "                    elif layers == 8:\n",
    "                        model.add(LSTM(num, return_sequences=True))\n",
    "                        model.add(LSTM(num, return_sequences=True))\n",
    "                        model.add(LSTM(num, return_sequences=True))\n",
    "                        model.add(LSTM(num, return_sequences=True))\n",
    "                        model.add(LSTM(num, return_sequences=True))\n",
    "                        model.add(LSTM(num, return_sequences=True))\n",
    "                    model.add(LSTM(num))\n",
    "                    model.add(Dense(129))\n",
    "                    model.compile(loss='mae', optimizer=optimizer)\n",
    "\n",
    "                    # fit model\n",
    "                    history = model.fit(X, sub_y, batch_size=batch, epochs=epoch, validation_data=(X_val, sub_y_val), shuffle=False)\n",
    "                    \n",
    "                    model_hash = \"model\" + \"_\".join(str(datetime.datetime.now()).split()) + \"_optimizer:\" + str(optimizer) + \"_epochs:\" + str(epoch) + \"_batches:\" + str(batch) + \"_units:\" + str(num) + \"_layers:\" + str(layers)\n",
    "                    \n",
    "                    # evaluate model by MAE\n",
    "\n",
    "                    loss = model.evaluate(X_test, sub_y_test, verbose=0)\n",
    "                    loss_string = 'MAE: %f' % loss\n",
    "\n",
    "                    # compared predicted request set \"app bias\" with actual \"app bias\"\n",
    "\n",
    "                    app_bias_list = get_app_bias_error(X_test, sub_y_test, model)\n",
    "\n",
    "                    app_bias_df = pd.DataFrame(app_bias_list)\n",
    "\n",
    "                    app_bias_mean = app_bias_df.mean().mean()\n",
    "                    \n",
    "                    # save model data in dictionary\n",
    "                    \n",
    "                    model_dict[model_hash] = {\n",
    "                        \"model-json\": model.to_json(),\n",
    "                        \"model-history\": history.history,\n",
    "                        \"MAE-holdout-set\": loss_string,\n",
    "                        \"app-bias-mean-holdout-set\": app_bias_mean\n",
    "                    }\n",
    "                    \n",
    "                    print(model_dict)\n",
    "                    \n",
    "                    results.append(model_dict)\n",
    "\n",
    "with open('results.pkl', 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
