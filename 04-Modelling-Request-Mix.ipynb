{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from random import randint\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from random import randint\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "\n",
    "from math import sin\n",
    "from math import pi\n",
    "from math import exp\n",
    "from random import randint\n",
    "from random import uniform\n",
    "\n",
    "from numpy import array\n",
    "from math import ceil\n",
    "from math import log10\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import RepeatVector\n",
    "\n",
    "from random import random\n",
    "from numpy import cumsum\n",
    "from numpy import array_equal\n",
    "from keras.layers import Bidirectional\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import re\n",
    "\n",
    "pd.set_option('display.max_columns', 999)\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './Datasets/'\n",
    "model_path = './Models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in config file - column list\n",
    "\n",
    "cumsum_cols_p = open(path + 'cumsum_cols.pkl', 'rb')\n",
    "cumsum_cols = pickle.load(cumsum_cols_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval transformation functions\n",
    "\n",
    "def most_common(lst): \n",
    "    return max(set(lst), key = lst.count)\n",
    "\n",
    "def reveal_bias(url):\n",
    "    path = url.split('/', 1)[1]\n",
    "    apps = re.sub('[/-]', ' ',path).split()[:-1]\n",
    "    return most_common(apps)\n",
    "\n",
    "def add_bias_labels(col_lst):\n",
    "    for i in range(len(col_lst)):\n",
    "        col_lst[i] = col_lst[i] + \"-bias-\" + reveal_bias(col_lst[i])\n",
    "    return col_lst\n",
    "\n",
    "def create_reqset_dict(col_list, req_list):\n",
    "    reqset_dict = dict(zip(add_bias_labels(col_list),req_list))\n",
    "    return reqset_dict\n",
    "\n",
    "def get_reqset_bias(dictionary):\n",
    "    bias_2 = sum([v for k,v in dictionary.items() if \"-bias-2\" in k])\n",
    "    bias_3 = sum([v for k,v in dictionary.items() if \"-bias-3\" in k])\n",
    "    bias_4 = sum([v for k,v in dictionary.items() if \"-bias-4\" in k])\n",
    "    \n",
    "    return (bias_2, bias_3, bias_4)\n",
    "\n",
    "def get_app_bias_error(X_test, y_test, model):\n",
    "    error_vectors_app_bias = list()\n",
    "    for i in range(len(X_test)):\n",
    "        predict_dict = create_reqset_dict(cumsum_cols, model.predict(X_test[i].reshape(1,1000,129))[0])\n",
    "        predict_bias = get_reqset_bias(predict_dict)\n",
    "\n",
    "        actual_dict = create_reqset_dict(cumsum_cols, y_test[i])\n",
    "        actual_bias = get_reqset_bias(actual_dict)\n",
    "\n",
    "        error_vectors_app_bias.append(np.absolute(np.array(actual_bias) - np.array(predict_bias)))\n",
    "        print(\"step \" + str(i) + \"done\")\n",
    "        print(str(len(X_test) - i) + \" steps remaining\")\n",
    "    return error_vectors_app_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in prepared datasets\n",
    "\n",
    "X_val_p = open(path + 'X_val.pkl', 'rb')\n",
    "X_val = pickle.load(X_val_p)\n",
    "\n",
    "sub_y_val_p = open(path + 'y_val.pkl', 'rb')\n",
    "sub_y_val = pickle.load(sub_y_val_p)\n",
    "\n",
    "X_train_p = open(path + 'X_train.pkl', 'rb')\n",
    "X = pickle.load(X_train_p)\n",
    "\n",
    "y_train_p = open(path + 'y_train.pkl', 'rb')\n",
    "sub_y = pickle.load(y_train_p)\n",
    "\n",
    "X_test_p = open(path + 'X_test.pkl', 'rb')\n",
    "X_test = pickle.load(X_test_p)\n",
    "\n",
    "y_test_p = open(path + 'y_test.pkl', 'rb')\n",
    "sub_y_test = pickle.load(y_test_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model - based on chosen model from grid search\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(125, return_sequences=True, input_shape=(1000, 129)))\n",
    "model.add(LSTM(125, return_sequences=True))\n",
    "model.add(LSTM(125))\n",
    "model.add(Dense(129))\n",
    "model.compile(loss='mae', optimizer=keras.optimizers.Adam(lr=0.01))\n",
    "\n",
    "# fit model\n",
    "\n",
    "history = model.fit(X, sub_y, batch_size=128, epochs=1, validation_data=(X_val, sub_y_val), shuffle=False)\n",
    "\n",
    "# fit model\n",
    "history = model.fit(X, sub_y, batch_size=32, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model by MAE\n",
    "\n",
    "loss = model.evaluate(X_test, sub_y_test, verbose=0)\n",
    "loss_string = 'MAE: %f' % loss\n",
    "\n",
    "# compared predicted request set \"app bias\" with actual \"app bias\"\n",
    "\n",
    "app_bias_list = get_app_bias_error(X_test, sub_y_test, model)\n",
    "\n",
    "app_bias_df = pd.DataFrame(app_bias_list)\n",
    "\n",
    "app_bias_mean = app_bias_df.mean().mean()\n",
    "\n",
    "# save model data in dictionary\n",
    "\n",
    "model_dict = {\n",
    "    \"model-json\": model.to_json(),\n",
    "    \"model-history\": history.history,\n",
    "    \"MAE-holdout-set\": loss_string,\n",
    "    \"app-bias-mean-holdout-set\": app_bias_mean,\n",
    "    \"model-object\": model\n",
    "}\n",
    "\n",
    "print(model_dict)\n",
    "\n",
    "with open(model_path + 'LSTM_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".micro",
   "language": "python",
   "name": ".micro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
