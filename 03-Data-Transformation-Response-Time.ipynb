{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "from random import randint\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from random import randint\n",
    "\n",
    "from math import sin\n",
    "from math import pi\n",
    "from math import exp\n",
    "from random import randint\n",
    "from random import uniform\n",
    "\n",
    "from numpy import array\n",
    "from math import ceil\n",
    "from math import log10\n",
    "\n",
    "from random import random\n",
    "from numpy import cumsum\n",
    "from numpy import array_equal\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import re\n",
    "\n",
    "pd.set_option('display.max_columns', 999)\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './Datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df = pd.read_parquet(path + \"logs.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT!! - deleting all logs without a response time (errors) - this loses key information but should simplify the problem while preserving the basic relationship\n",
    "\n",
    "log_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature transformations\n",
    "\n",
    "# Convert resp_time and bytes_setn into float\n",
    "\n",
    "log_df.resp_time = log_df.resp_time.astype(\"float\")\n",
    "log_df.bytes_sent = log_df.bytes_sent.astype(\"float\")\n",
    "\n",
    "# Convert datetime string into Pandas Datetime\n",
    "\n",
    "log_df.datetime = pd.to_datetime(log_df.datetime,\n",
    "                                    format=\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Create categorical variables\n",
    "\n",
    "log_df = pd.get_dummies(log_df, columns = ['resp_code'])\n",
    "log_df = pd.get_dummies(log_df, columns = ['url'])\n",
    "\n",
    "# Create column lists\n",
    "\n",
    "url_features = [col for col in log_df.columns if \"url\" in col]\n",
    "app_cols = [col for col in log_df.columns if \"app_\" in col]\n",
    "resp_codes = [col for col in log_df.columns if \"resp_code\" in col]\n",
    "\n",
    "print(log_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequence prediction outcome variable -> next 250 requests\n",
    "\n",
    "for url in url_features:\n",
    "     log_df[\"CS_\" + url] = log_df[url].rolling(250).sum()\n",
    "\n",
    "cumsum_cols = [col for col in log_df.columns if \"CS_\" in col]\n",
    "\n",
    "# Create rolling average request response time (over the next 250 requests)\n",
    "\n",
    "log_df[\"av_rolling_resp_time_250\"] = log_df[\"resp_time\"].rolling(250).sum() / 250\n",
    "\n",
    "# Delete NaN rows\n",
    "\n",
    "log_df.dropna(inplace=True)\n",
    "\n",
    "# Shift request_mix and response_time variables 250 places down\n",
    "\n",
    "log_df[cumsum_cols] = log_df[cumsum_cols].shift(-250)\n",
    "log_df[\"av_rolling_resp_time_250\"] = log_df[\"av_rolling_resp_time_250\"].shift(-250)\n",
    "#log_df[\"error_rate\"] = log_df[\"error_rate\"].shift(-250)\n",
    "\n",
    "# Delete NaN rows again\n",
    "\n",
    "log_df.dropna(inplace=True)\n",
    "\n",
    "# Scale key cols between 0 and 1\n",
    "\n",
    "log_df[cumsum_cols] = MinMaxScaler().fit_transform(log_df[cumsum_cols])\n",
    "log_df[\"av_rolling_resp_time_250\"] = MinMaxScaler().fit_transform(log_df[[\"av_rolling_resp_time_250\"]])\n",
    "log_df[app_cols] = MinMaxScaler().fit_transform(log_df[app_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = url_features + resp_codes + [\"resp_time\", \"bytes_sent\", \"datetime\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df.to_parquet(path + 'resp_df.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".micro",
   "language": "python",
   "name": ".micro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
